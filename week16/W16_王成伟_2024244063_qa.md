---
title: [周次16--第三章]高级算法大模型辅助学习
author:
    - name: 王成伟^[2024244063,计算机科学与技术]
---

**Q01：深度学习回归与传统回归方法的对比**

- prompt：深度学习回归与传统回归方法（如线性回归、SVM回归）相比有什么优势？

- model_name：ChatGPT

- model_version：3.5-Turbo

- response：

  - 深度学习回归与传统回归方法（如线性回归、支持向量机回归（SVM回归））相比，具有以下几个主要优势：

    1. **非线性建模能力**

    - **传统回归方法：** 线性回归假设输入和输出之间存在线性关系，SVM回归通过核函数（如RBF核）可以扩展到非线性关系，但本质上仍然基于一种固定的映射（通过核函数映射到高维空间）。
    - **深度学习回归：** 深度学习模型（尤其是神经网络）能够自动学习输入数据中的复杂非线性关系。它通过多层神经网络结构，可以捕捉到非常复杂的模式和特征，这对于大多数现实世界的数据尤其重要。
  
    2. **特征自动提取**
  
    - **传统回归方法：** 需要手动选择和提取特征，特征工程依赖于领域知识和数据预处理，复杂的特征组合可能难以发现。
    - **深度学习回归：** 深度学习模型能够通过网络的多层结构自动学习数据的高阶特征，这减少了人工特征工程的工作量，并能发现更隐蔽和复杂的模式。

    3. **适应大规模数据集**
  
    - **传统回归方法：** 对于小规模数据集，传统回归方法可以有效使用。然而，当数据集变得庞大时，这些方法往往会面临性能瓶颈，尤其在特征维度较高时。
    - **深度学习回归：** 深度学习模型在处理大规模数据集时表现尤为出色，尤其是当数据量非常大时，深度神经网络通过其大量的参数可以提取出丰富的信息，提升预测精度。
  
    4. **对噪声的鲁棒性**
  
    - **传统回归方法：** 线性回归等方法对噪声较为敏感，尤其是当噪声较大时，预测性能可能会大幅下降。SVM回归可以通过调整参数如C和核函数来改善这一问题，但它的调参也较为复杂。
    - **深度学习回归：** 虽然深度学习模型也可能受到噪声的影响，但通过适当的正则化（如Dropout、L2正则化等），它们通常能更好地从噪声中学习有用的模式。深度学习还可以通过训练更复杂的模型来对抗噪声。
  
    5. **模型表达能力**
  
    - **传统回归方法：** 线性回归只能拟合线性关系，SVM回归虽然能够处理一些非线性问题，但其能力依赖于所选的核函数和超参数。
    - **深度学习回归：** 深度神经网络通过增加网络的层数和节点数，可以拟合几乎任何形式的函数，从而具有更强的表达能力。
  
    6. **端到端训练**
  
    - **传统回归方法：** 传统回归方法一般依赖于显式的训练过程，需要单独的模型设计和优化步骤。
    - **深度学习回归：** 深度学习回归可以实现端到端的训练，这意味着输入数据到预测结果的过程可以在一个统一的框架中完成，所有参数都可以通过反向传播和梯度下降优化。
  
    总结：
  
    深度学习回归相较于传统回归方法，主要优势在于其强大的非线性建模能力、自动特征提取能力、处理大规模数据的能力、较强的鲁棒性以及模型表达能力。尽管深度学习模型需要较大的数据量、计算资源和更长的训练时间，但在复杂任务和大数据场景下，它通常能提供显著更高的预测精度。

**Q02：SVM回归的核函数选择**

- prompt：SVR如何选择合适的核函数（如线性核、RBF核等）
- model_name：ChatGPT
- model_version：3.5-Turbo
- response：
  - 在支持向量回归（SVR）中，选择合适的核函数是影响模型性能的关键因素之一。核函数决定了数据点在高维空间中的映射方式，并影响到模型的预测能力。常见的核函数包括线性核、径向基函数（RBF）核、多项式核等，每种核函数适用于不同的数据特性。以下是如何选择合适核函数的一些指导原则：
  
     1. **线性核（Linear Kernel）**
  
     - **公式：** 
       $$
       K(x,x′)=x⋅x′K(x, x') = x \cdot x'K(x,x′)=x⋅x′
       $$
       
  
     - 适用场景：
  
       - **数据呈现线性关系**：如果数据在原始空间中已经接近线性可分或呈线性趋势，使用线性核是一个合理选择。
       - **高效计算**：线性核的计算开销较低，适用于高维数据和大型数据集。
  
     - 如何选择：
  
       - 当数据的特征之间没有显著的非线性关系时，可以选择线性核。
       - 使用线性核时，可以通过训练误差（如R²或均方误差）来判断模型的表现是否足够好。如果性能不好，可以考虑使用其他核函数。
  
     2. **径向基函数核（RBF Kernel）**
  
     - **公式：** 
       $$
       K(x,x′)=exp⁡(−∥x−x′∥22σ2)K(x, x') = \exp \left( - \frac{\|x - x'\|^2}{2\sigma^2} \right)K(x,x′)=exp(−2σ2∥x−x′∥2)
       $$
       
  
     - 适用场景：
  
       - **数据存在复杂的非线性关系**：RBF核能够捕捉输入特征之间的非线性关系，特别适合在数据集具有复杂结构时使用。
       - **通用选择**：RBF核通常是最常用的核函数，适用于大多数实际问题，特别是在没有明显先验知识时。
  
     - 如何选择：
  
       - RBF核通常表现良好，尤其在数据存在不规则的非线性模式时。
       - RBF核的性能依赖于两个超参数：**C**（惩罚参数）和**gamma**（核宽度）。**C**控制模型的复杂度，**gamma**则控制RBF核的“宽度”，决定了支持向量的影响范围。
       - 选择合适的gamma值非常关键，**gamma**太大时模型容易过拟合，**gamma**太小时模型可能无法捕捉数据的复杂性。
  
     3. **多项式核（Polynomial Kernel）**
  
     - **公式：** 
       $$
       K(x,x′)=(x⋅x′+r)dK(x, x') = (x \cdot x' + r)^dK(x,x′)=(x⋅x′+r)d
       $$
       
  
     - 适用场景：
  
       - **数据具有某种多项式关系**：当数据的特征之间存在多项式（例如二次或三次多项式）关系时，使用多项式核可能会有很好的效果。
       - **适中复杂度**：多项式核提供了线性和非线性之间的折中，可以调整多项式的次数（degree）来控制模型的复杂度。
  
     - 如何选择：
  
       - 通过交叉验证或网格搜索来选择合适的多项式次数（d）和常数项（r）。
       - 如果数据没有明显的多项式模式，可能不需要使用此核。
  
     4. **Sigmoid核（Sigmoid Kernel）**
  
     - **公式：** 
       $$
       K(x,x′)=tanh⁡(αx⋅x′+c)K(x, x') = \tanh(\alpha x \cdot x' + c)K(x,x′)=tanh(αx⋅x′+c)
       $$
       
  
     - 适用场景：
  
       - **数据存在类似神经网络的关系**：Sigmoid核通常与神经网络的激活函数（如tanh）类似，用于一些复杂的模式识别任务。
       - **一般不常用**：在实际应用中，Sigmoid核较少使用，尤其是在SVR中，因为其表现往往不如RBF核。
  
     5. **核函数选择的策略**
  
     - **经验性选择：** 如果你对数据没有强烈的先验知识，可以从RBF核开始。RBF核在大多数情况下表现较好，尤其适用于数据具有非线性关系的情况。
     - **网格搜索与交叉验证：** 使用**网格搜索**结合**交叉验证**来选择核函数及其相应的超参数。通过不同核函数的比较，可以评估哪个核函数在给定数据集上表现最佳。
     - **特征的直观理解：** 如果数据的特征之间有明确的线性关系，线性核可能是最合适的选择。如果你认为特征之间可能存在复杂的关系，RBF核可能更适用。
     - **计算资源与时间：** 线性核通常计算开销较小，适用于较大的数据集。RBF核可能计算较为昂贵，尤其是在数据量很大的时候。
  
     6. **常见实践**
  
     - **开始使用RBF核：** 对于大多数任务，RBF核通常是一个好的起点，特别是在没有明确的先验假设时。
     - **调参：** 无论选择哪种核函数，优化超参数（如**C**、**gamma**）至关重要。可以使用网格搜索（Grid Search）或随机搜索（Random Search）来寻找最优超参数。
     - **交叉验证：** 使用交叉验证来评估模型性能，选择最适合数据集的核函数和超参数。
  
     总结：
  
     - **线性核：** 适用于数据之间存在线性关系的情况，计算开销低。
     - **RBF核：** 适用于复杂的非线性数据，广泛使用并且表现通常较好。
     - **多项式核：** 当数据具有多项式关系时可以尝试，适用于某些特定场景。
     - **Sigmoid核：** 很少使用，通常不推荐作为默认选择。
  
     选择合适的核函数通常需要根据数据的特性和模型的表现来决定，结合交叉验证来优化模型性能。

**Q03：深度学习回归过拟合问题**

- prompt：如何避免深度学习回归模型出现过拟合？常用的正则化方法有哪些？

- model_name：ChatGPT

- model_version：3.5-Turbo

- response：

  - 在深度学习回归模型中，过拟合是指模型在训练数据上表现得非常好，但在测试数据或新数据上的泛化能力差。为了避免过拟合，通常采用一系列正则化技术。以下是一些常用的正则化方法及其避免过拟合的原理：

    1. **L2 正则化（Ridge 正则化）**
  
    - **原理：** L2 正则化通过在损失函数中加入所有权重参数的平方和（即权重的L2范数）来惩罚过大的权重值。这样可以限制模型过于依赖某些特征，促使模型在所有特征上做出更均衡的预测。
  
    - **公式：** 
      $$
      L(θ)=Loriginal(θ)+λ∑iθi2L(\theta) = L_{\text{original}}(\theta) + \lambda \sum_i \theta_i^2L(θ)=Loriginal(θ)+λi∑θi2 
      $$
      其中，λ是正则化强度超参数，控制正则化的惩罚程度。
  
    - **效果：** 降低模型对训练集的依赖，防止模型复杂度过高。
  
    2. **L1 正则化（Lasso 正则化）**
  
    - **原理：** L1 正则化通过在损失函数中加入权重的绝对值和（即L1范数）来惩罚过大的权重。这种正则化方法的特点是可以将某些权重完全归零，从而实现特征选择，进而提升模型的稀疏性。
  
    - **公式：**
      $$
      L(θ)=Loriginal(θ)+λ∑i∣θi∣L(\theta) = L_{\text{original}}(\theta) + \lambda \sum_i |\theta_i|L(θ)=Loriginal(θ)+λi∑∣θi∣
      $$
      
  
    - **效果：** 降低特征冗余，选择最重要的特征。L1 正则化比 L2 更倾向于产生稀疏的模型（即一些权重会被压缩到零）。
  
    3. **L1 和 L2 正则化（Elastic Net）**
  
    - **原理：** Elastic Net 是 L1 和 L2 正则化的组合，它同时具有 L1 的特征选择能力和 L2 的平滑能力。通过平衡两个正则化项的比例，Elastic Net 能够在特征选择和模型稳定性之间找到最佳折中。
  
    - **公式：** 
      $$
      L(θ)=Loriginal(θ)+λ1∑i∣θi∣+λ2∑iθi2L(\theta) = L_{\text{original}}(\theta) + \lambda_1 \sum_i |\theta_i| + \lambda_2 \sum_i \theta_i^2L(θ)=Loriginal(θ)+λ1i∑∣θi∣+λ2i∑θi2
      $$
      
  
    - **效果：** 提供了比单独使用 L1 或 L2 更强的正则化能力。
  
    4. **Dropout**
  
    - **原理：** Dropout 是一种非常有效的防止过拟合的方法。在训练过程中，随机丢弃（置零）网络中某些神经元的输出，使得网络在每次训练时都会有一个不同的子网络。这可以防止神经网络对某些神经元过度依赖，从而提高模型的泛化能力。
    - **公式：**
      在每次训练时，某些神经元的激活值被设置为零，丢弃的概率由超参数 ppp 决定。
    - **效果：** 使得模型更具鲁棒性，减少过拟合，通常在训练时设置为 0.2 至 0.5。
  
    5. **提前停止（Early Stopping）**
  
    - **原理：** 在训练过程中，随着训练次数的增加，训练误差通常会下降，但验证误差可能会在某一点后开始上升。提前停止是在验证误差开始上升时停止训练，从而避免过度训练导致的过拟合。
    - **效果：** 通过监控验证集的误差，确保模型不会过度拟合训练集。
  
    6. **数据增强（Data Augmentation）**
  
    - **原理：** 数据增强是通过对训练数据进行转换（如旋转、平移、翻转等），人工增加训练数据的多样性。对于回归问题，数据增强方法可以包括对输入特征添加噪声、随机采样或合成新的数据点等。
    - **效果：** 增加训练数据的多样性，提升模型的泛化能力。
  
    7. **权重衰减（Weight Decay）**
  
    - **原理：** 权重衰减类似于 L2 正则化，但它是通过在每次参数更新时对权重进行衰减来实现的。它的目的是控制模型的复杂度，防止参数过大。
    - **效果：** 限制模型复杂度，防止过拟合。
  
    8. **批归一化（Batch Normalization）**
  
    - **原理：** 批归一化通过标准化每一层的输入，使其均值接近零，方差接近一，进而加速训练过程并使模型更稳定。这不仅有助于提高训练速度，还能减少由于梯度消失或梯度爆炸导致的过拟合。
    - **效果：** 提高训练稳定性，减少模型对初始权重的敏感性，从而避免过拟合。
  
    9. **增加数据量（更多训练数据）**
  
    - **原理：** 通过增加更多的训练数据，可以使模型学习到更丰富的数据模式，从而减少过拟合的风险。数据量的增加可以通过采集更多数据或利用数据增强技术实现。
    - **效果：** 更大的训练集可以提供更多的变异性，减少模型对噪声的过度拟合。
  
    10. **模型集成（Ensemble Methods）**
  
    - **原理：** 集成方法通过将多个模型的预测结果结合起来（如随机森林、梯度提升树等），降低单一模型过拟合的风险。通过组合多个弱学习器，集成方法通常可以提升泛化能力。
    - **效果：** 提高模型的鲁棒性，减少过拟合。
  
    11. **噪声注入（Noise Injection）**
  
    - **原理：** 在训练过程中，向输入数据、标签或网络权重添加噪声，以增加数据的多样性并减少模型的过拟合倾向。
    - **效果：** 通过人为增加不确定性，促使模型学习更为稳健的特征，而不是仅仅适应训练集的噪声。
  
    总结
  
    避免深度学习回归模型过拟合的关键在于通过正则化方法来控制模型的复杂度和提升泛化能力。常用的正则化方法包括 L1 和 L2 正则化、Dropout、提前停止、数据增强、批归一化等。选择适当的正则化技术时，需要根据具体任务和数据的特点进行调整，并通过交叉验证来调优超参数。

**Q04：深度学习回归模型的优化器**

- prompt：在训练深度学习回归模型时，如何选择合适的优化器（如Adam、SGD等）？

- model_name：ChatGPT

- model_version：3.5-Turbo

- response：

  - 在训练深度学习回归模型时，选择合适的优化器是提高模型性能、加速训练并避免陷入局部最优的关键因素之一。不同的优化器有不同的特点，适用于不同类型的任务和数据。以下是几种常见优化器（如Adam、SGD等）的比较，以及如何选择适合的优化器：

    1. **随机梯度下降（SGD）**
    
    - **原理：** SGD是最基础的优化器，在每一步更新时使用一个样本（或一小批样本）来计算梯度。更新公式为： 
      $$
      θ=θ−η∇θL(θ)\theta = \theta - \eta \nabla_\theta L(\theta)θ=θ−η∇θL(θ)
      $$
      其中，η\etaη 是学习率，∇θL(θ)\nabla_\theta L(\theta)∇θL(θ) 是损失函数关于模型参数的梯度。
    
    - 优点：
    
      - 简单、易于实现。
      - 适用于较为简单的模型和较小的数据集。
    
    - 缺点：
    
      - 收敛速度慢，容易受到学习率的影响。
      - 在复杂任务或大规模数据集上可能表现不佳，容易陷入局部最优解。
    
    - 适用场景：
    
      - 当模型比较简单、数据量不大且对计算资源要求不高时，可以选择SGD。
      - 在使用SGD时，可以通过学习率衰减和动量（Momentum）来加速收敛并避免震荡。
    
    2. **带动量的随机梯度下降（SGD with Momentum）**
    
    - **原理：** 在标准SGD的基础上引入了动量概念，通过引入过去梯度的累积值来加速收敛，减少梯度更新的震荡。更新公式为： 
      $$
      vt=βvt−1+(1−β)∇θL(θ)v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta L(\theta)vt=βvt−1+(1−β)∇θL(θ) θ=θ−ηvt\theta = \theta - \eta v_tθ=θ−ηvt
      $$
      其中，β\betaβ 是动量因子，通常设置为0.9。
    
    - 优点：
    
      - 减少了梯度的震荡，加速了收敛，尤其在高维空间中。
    
    - 适用场景：
    
      - 对于大部分的回归任务，尤其是当数据具有大量特征时，SGD + Momentum 是一个很好的选择。
      - 如果训练过程中的梯度更新不稳定，可以尝试使用带动量的SGD。
    
    3. **AdaGrad**
    
    - **原理：** AdaGrad通过自适应地调整每个参数的学习率来提高训练效率。对于稀疏特征，AdaGrad会自动提高学习率，而对于常规特征则降低学习率。更新公式为： 
      $$
      θ=θ−ηGt+ϵ∇θL(θ)\theta = \theta - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta L(\theta)θ=θ−Gt+ϵη∇θL(θ) 其中，GtG_tGt
      $$
       是梯度的累积平方和，ϵ 是防止除零的常数。
    
    - 优点：
    
      - 对稀疏数据（如自然语言处理中的词袋模型）有良好的效果。
      - 适用于需要自适应调整学习率的场景。
    
    - 缺点：
    
      - 学习率在训练过程中会逐渐减小，导致后期学习效率较低。
    
    - 适用场景：
    
      - 当数据包含稀疏特征时（例如文本数据、推荐系统中的稀疏矩阵），AdaGrad可能是一个不错的选择。
      - 一般不推荐用于回归任务中的复杂深度神经网络模型。
    
    4. **RMSProp**
    
    - **原理：** RMSProp是AdaGrad的改进，它通过引入指数加权平均来限制累积的梯度平方，防止学习率衰减过快。更新公式为： 
      $$
      vt=βvt−1+(1−β)∇θL(θ)2v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta L(\theta)^2vt=βvt−1+(1−β)∇θL(θ)2 θ=θ−ηvt+ϵ∇θL(θ)\theta = \theta - \frac{\eta}{\sqrt{v_t + \epsilon}} \nabla_\theta L(\theta)θ=θ−vt+ϵη∇θL(θ)
      $$
      
    
    - 优点：
    
      - 适用于非平稳目标函数（即目标函数随时间变化较大）。
      - 比AdaGrad更稳定，适合用于动态数据和大规模问题。
    
    - 适用场景：
    
      - 当数据具有较大的波动性或动态变化时，RMSProp表现较好，特别是对于深度学习中的复杂任务。
    
    5. **Adam（Adaptive Moment Estimation）**
    
    - **原理：** Adam结合了动量和RMSProp的优点，使用一阶矩（梯度的平均值）和二阶矩（梯度的平方的平均值）来动态调整每个参数的学习率。更新公式为： 
      $$
      mt=β1mt−1+(1−β1)∇θL(θ)m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta L(\theta)mt=β1mt−1+(1−β1)∇θL(θ) vt=β2vt−1+(1−β2)∇θL(θ)2v_t = \beta_2 v_{t-1} + (1 - \beta_2) \nabla_\theta L(\theta)^2vt=β2vt−1+(1−β2)∇θL(θ)2 m^t=mt1−β1t,v^t=vt1−β2t\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}m^t=1−β1tmt,v^t=1−β2tvt θ=θ−ηv^t+ϵm^t\theta = \theta - \frac{\eta}{\sqrt{\hat{v}_t + \epsilon}} \hat{m}_tθ=θ−v^t+ϵηm^t
      $$
      
    
    - 优点：
    
      - 自动调整学习率，通常可以在不同的任务中达到较好的效果。
      - 非常适合用于大规模数据和复杂的深度学习模型。
    
    - 缺点：
    
      - 在某些任务中，Adam可能会过拟合（尤其是在训练数据量不够大时）。
    
    - 适用场景：
    
      - Adam是当前最常用的优化器，适用于大多数深度学习回归任务。它能自动适应学习率，适用于大数据集、复杂模型、复杂损失函数等。
    
    6. **Nadam（Nesterov-accelerated Adaptive Moment Estimation）**
    
    - **原理：** Nadam结合了Nesterov动量和Adam的思想，在计算梯度更新时使用了Nesterov加速。更新公式与Adam类似，但动量部分考虑了Nesterov的优化思想。
    - 优点：
      - 结合了Nesterov动量的优势，可以加速训练过程并避免过拟合。
    - 适用场景：
      - 当Adam表现不佳时，尝试Nadam可能会得到更好的结果。
    
    7. **选择合适的优化器的建议**
    
    - **初学者：** 如果你不确定使用哪个优化器，**Adam**是一个很好的起点，它通常适用于各种类型的深度学习任务，能够自动调整学习率，减少调参工作。
    - **较小数据集或简单模型：** 对于简单的回归问题（例如线性回归或浅层神经网络），**SGD**（带动量）通常能够提供良好的效果。通过调整学习率和动量参数，可以获得理想的收敛速度。
    - **稀疏数据：** 对于具有稀疏特征的数据（例如文本数据），**AdaGrad**或**RMSProp**可能会表现得更好。
    - **复杂模型或大数据：** 如果模型非常复杂，或者数据量很大，使用**Adam**或**Nadam**通常能取得更好的训练效果。
    
    总结
    
    - **Adam**：适用于大多数深度学习任务，自动调整学习率，通常效果较好。
    - **SGD**：适用于较简单的模型和数据，较慢但稳定。
    - **RMSProp**：适用于动态变化的损失函数和复杂模型。
    - **AdaGrad**：适用于稀疏特征的数据，但学习率会迅速衰减。
    - **Nadam**：结合了Adam和Nesterov加速，可能在某些任务中提供更好的收敛速度。
    
    选择优化器时，建议根据任务的复杂性、数据特性以及计算资源等因素进行综合考虑。通过实验和交叉验证来调优优化器及其超参数，通常可以找到最佳的优化策略。

**Q05：深度学习回归处理多模态数据 **

- prompt：深度学习回归在处理多模态数据（如图像、文本和结构化数据混合）时，如何设计有效的网络架构来提高模型的准确性和鲁棒性？

- model_name：ChatGPT

- model_version：3.5-Turbo

- response：

  - 在深度学习回归任务中，处理**多模态数据**（如图像、文本和结构化数据混合）是一个挑战性问题。不同模态的数据具有不同的特征和结构，因此，设计有效的网络架构以融合这些数据并提高模型的准确性和鲁棒性是关键。

    1. **理解多模态数据的特性**
    
    - **图像**：通常是高维数据，需要卷积神经网络（CNN）来提取空间特征。
    - **文本**：通常是序列数据，需要循环神经网络（RNN）、长短时记忆网络（LSTM）或Transformer等模型来提取语义特征。
    - **结构化数据**：通常是低维度的表格数据，直接用于模型的全连接层进行特征提取。
    
    这些不同类型的数据必须有效地融合，以便在回归任务中利用所有模态的信息。常见的设计方案可以包括特征级融合、决策级融合以及多模态自适应网络等。
    
    2. **常用的多模态数据融合方法**
    
    A. **特征级融合（Feature-level Fusion）**
    
    特征级融合是最常见的多模态数据融合方法，通过将不同模态的数据特征在网络的某一层（通常是早期层）进行组合。每个模态的数据通常会单独通过相应的网络进行处理，然后将它们的表示合并为一个共享的特征向量，再进行后续的回归任务。
    
    - **图像模态处理**：使用卷积神经网络（CNN）处理图像数据，提取图像的高级特征。
    - **文本模态处理**：使用LSTM、GRU、BERT等模型来提取文本的语义特征。
    - **结构化数据处理**：使用全连接层或嵌入层处理结构化数据。
    
    具体架构设计：
    
    1. **单独处理每种模态**：每种模态的数据通过独立的子网络进行处理。例如，图像通过CNN网络，文本通过LSTM或Transformer，结构化数据通过全连接网络。
    
    2. **拼接或加权融合**：各个子网络的输出（通常是特征向量）可以通过拼接（concatenation）、加权平均、加法等方式进行融合。拼接方式将各模态的特征向量直接拼接成一个长的向量，然后传递到后续的全连接层。
    
    3. **后续回归任务**：将融合后的特征输入到一个共享的回归层，进行最终的预测。
    
    4. B. **决策级融合（Decision-level Fusion）**
    
       决策级融合是在各个模态分别训练的模型的输出上进行融合。每种模态的模型单独生成预测结果，然后将这些预测结果结合在一起，最终做出一个回归预测。
    
       - **独立训练**：每个模态的数据都通过独立的模型进行回归。
       - **结果融合**：各个回归模型的输出通过加权平均、投票或其他方法进行融合。
    
       例如，可以使用各个模态的预测值 y1,y2,y3y_1, y_2, y_3y1,y2,y3：
       $$
       yfinal=αy1+βy2+γy3y_{\text{final}} = \alpha y_1 + \beta y_2 + \gamma y_3yfinal=αy1+βy2+γy3
       $$
       其中，α,β,γ 是权重，表示不同模态在最终预测中的贡献。
    
       C. **多模态自适应网络（Multi-modal Adaptive Networks）**
    
       这种方法通过自适应机制学习如何在不同模态之间选择最重要的特征和信息。自适应的机制可能会在不同的训练阶段调整对每种模态的依赖程度，以适应多变的输入。
    
       - **Attention机制**：使用多模态注意力机制（例如，Self-attention或Cross-attention）来自动为每个模态分配不同的权重，重点关注最相关的特征。
       - **跨模态学习**：通过共享某些层或通过跨模态的连接来促进模态之间的交互。这样，模型可以学习到模态之间的相关性，并通过融合重要的信息进行回归预测。
